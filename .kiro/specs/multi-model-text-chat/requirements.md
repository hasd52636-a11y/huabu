# 需求文档

## 介绍

本文档规定了在右侧边栏文本聊天中实现多模型选择功能的需求。该功能将允许用户从多个文本生成模型（Gemini 3.0系列、GPT-4-all、GPT-4o-all、GPT-5-nano）中进行选择，同时保持对话历史记录并确保与现有功能的向后兼容性。

## 术语表

- **文本聊天系统**: 在CaocaoAIChat组件中实现的右侧边栏聊天界面
- **模型选择器**: 允许用户在可用文本模型之间进行选择的UI组件
- **对话管理器**: 管理聊天历史记录和模型切换的系统组件
- **模型配置**: 定义可用模型及其设置的配置结构(NewModelConfig)
- **神马服务**: 处理所有模型文本生成请求的API服务
- **AI服务适配器**: 将请求路由到适当提供商的服务适配器

## 需求

### 需求1：模型选择界面

**用户故事：** 作为用户，我希望在侧边栏聊天中选择不同类型的文本模型，以便根据不同使用场景选择最合适的模型。

#### 验收标准

1. 当用户打开侧边栏文本聊天时，文本聊天系统应显示包含所有可用文本模型的模型选择下拉菜单
2. 当用户点击模型选择器时，模型选择器应按类型分组显示可用模型：
   - **快速轻量型**: gemini-3-flash-preview-nothinking、gemini-3-flash-preview
   - **深度分析型**: gemini-3-pro-preview-thinking、gpt-4-all、gpt-4o-all
   - **推理专用型**: gemini-3-pro-preview-thinking、gpt-5-nano-2025-08-07
   - **联网功能型**: gemini-3-pro-preview-thinking、gpt-4-all、gpt-4o-all
   - **全模态型**: gpt-4o-all、gemini-3-pro-preview-thinking（支持文本、图像、音频等多种输入输出）
3. 当用户选择不同模型时，文本聊天系统应更新当前模型并在UI中显示所选模型名称和类型标识
4. 当没有明确选择模型时，文本聊天系统应使用gemini-3-flash-preview作为默认模型
5. 模型选择器应显示用户友好的模型名称、类型标识和功能说明

### 需求2：智能模型推荐

**用户故事：** 作为用户，我希望系统能够根据我的使用场景智能推荐合适的模型类型，以便获得最佳的性能和成本平衡。

#### 验收标准

1. 当用户需要快速响应时，文本聊天系统应推荐快速轻量型模型（gemini-3-flash系列）
2. 当用户询问复杂问题时，文本聊天系统应推荐深度分析型模型（gpt-4系列）
3. 当用户需要逻辑推理时，文本聊天系统应推荐推理专用型模型（thinking系列、gpt-5-nano）
4. 当用户询问实时信息时，文本聊天系统应推荐联网功能型模型
5. 当用户需要处理多种媒体类型时，文本聊天系统应推荐全模态型模型（gpt-4o-all、gemini-3-pro-preview-thinking）
6. 文本聊天系统应在模型选择器中显示每种类型的特点和适用场景

### 需求3：对话历史保持

**用户故事：** 作为用户，我希望在切换模型时保持对话历史记录，以便能够在不同模型之间无缝继续对话。

#### 验收标准

1. 当用户在活跃对话期间切换模型时，对话管理器应保留聊天历史中的所有现有消息
2. 当用户在切换模型后发送新消息时，文本聊天系统应使用新选择的模型进行生成，同时保持对话上下文
3. 当显示消息时，文本聊天系统应指示哪个模型生成了每个响应
4. 当用户切换模型时，文本聊天系统应保持对话线程而不清除之前的消息
5. 对话管理器应将模型信息与每条消息一起存储以供将来参考

### 需求4：模型配置集成

**用户故事：** 作为系统管理员，我希望多模型聊天与现有的NewModelConfig结构集成，以便集中管理模型可用性和设置。

#### 验收标准

1. 当系统初始化时，文本聊天系统应从NewModelConfig结构中读取可用的文本模型
2. 当配置中禁用某个模型时，模型选择器应将该模型从可用选项中排除
3. 当模型凭据缺失或无效时，文本聊天系统应显示适当的错误消息并禁用受影响的模型
4. 文本聊天系统应对所有文本模型使用相同的神马服务API端点格式
5. 当配置发生更改时，文本聊天系统应更新可用模型列表而无需刷新页面

### 需求5：使用选定模型进行文本生成

**用户故事：** 作为用户，我希望我的消息由选定的模型处理，以便从我选择的特定AI模型获得响应。

#### 验收标准

1. 当用户发送消息时，文本聊天系统应使用当前选定的模型进行文本生成
2. 当调用文本生成API时，AI服务适配器应将选定的模型ID传递给神马服务
3. 当发出文本生成请求时，神马服务应在API请求中使用指定的模型ID
4. 当模型无法响应时，文本聊天系统应显示错误消息并允许用户重试或切换模型
5. 文本聊天系统应对所有支持的文本模型保持相同的API请求格式

### 需求6：用户界面集成

**用户故事：** 作为用户，我希望模型选择与现有聊天界面无缝集成，以便该功能感觉自然且不会干扰我的工作流程。

#### 验收标准

1. 当显示模型选择器时，文本聊天系统应将其显著地放置在聊天标题中，而不干扰现有控件
2. 当选择模型时，文本聊天系统应提供显示当前活动模型的视觉反馈
3. 当聊天界面加载时，模型选择器应清楚地显示当前选定的模型
4. 文本聊天系统应为所有新UI元素保持现有的深色/浅色主题支持
5. 模型选择器应支持与现有本地化一致的中文和英文语言界面

### 需求7：向后兼容性

**用户故事：** 作为开发者，我希望多模型功能保持向后兼容性，以便现有功能无需修改即可继续工作。

#### 验收标准

1. 当启用多模型功能时，文本聊天系统应继续支持所有现有聊天功能
2. 当没有明确选择模型时，文本聊天系统应使用NewModelConfig中的默认文本模型
3. 当遗留代码调用文本生成时，AI服务适配器应通过相同接口路由请求
4. 文本聊天系统应保持与现有语音和手势控制功能的兼容性
5. 当外部系统与聊天集成时，文本聊天系统应保留现有API接口

### 需求8：错误处理和验证

**用户故事：** 作为用户，我希望在模型选择或生成失败时获得清晰的错误消息，以便能够快速理解和解决问题。

#### 验收标准

1. 当选定的模型不可用时，文本聊天系统应显示用户友好的错误消息并建议替代模型
2. 当模型的API凭据无效时，文本聊天系统应显示配置错误消息和指导
3. 当文本生成请求失败时，文本聊天系统应允许用户使用相同模型重试或切换到不同模型
4. 当出现网络连接问题时，文本聊天系统应显示适当的离线指示器和重试选项
5. 文本聊天系统应记录所有模型选择和生成错误以供调试

### 需求9：智能模型路由修复

**用户故事：** 作为用户，我希望当我上传图像进行分析时，系统能够自动选择支持图像分析的模型，以便获得准确的分析结果。

#### 验收标准

1. 当用户上传图像进行分析时，文本聊天系统应自动检测到图像内容并选择支持多模态的模型
2. 当智能路由检测到图像内容时，文本聊天系统应优先选择全模态型模型（gemini-3-pro-preview-thinking、gpt-4o-all）
3. 当用户明确选择了支持图像分析的模型时，文本聊天系统应保持该模型选择而不被智能路由覆盖
4. 当模型不支持图像分析时，文本聊天系统应显示明确的错误提示并建议切换到支持的模型
5. 文本聊天系统应在界面中清楚显示当前选择的模型是否支持图像分析功能

### 需求10：联网模型支持

**用户故事：** 作为用户，我希望能够使用具有联网功能的模型，以便获得最新信息和实时数据。

#### 验收标准

1. 当用户选择联网模型时，文本聊天系统应在UI中明确标识该模型具有联网功能
2. 当使用联网模型生成响应时，文本聊天系统应允许模型访问实时网络信息
3. 当联网模型无法访问网络时，文本聊天系统应显示相应的警告信息
4. 文本聊天系统应为联网模型提供不同的图标或标识以区分普通模型
5. 当用户询问实时信息时，文本聊天系统应优先推荐使用联网模型

### 需求11：全模态模型支持

**用户故事：** 作为用户，我希望能够使用全模态模型处理多种类型的输入和输出，以便在一个对话中处理文本、图像、音频等多种媒体。

#### 验收标准

1. 当用户选择全模态模型时，文本聊天系统应支持文本、图像等多种输入类型
2. 当使用全模态模型时，文本聊天系统应在UI中显示该模型的多媒体处理能力
3. 当用户上传图像或其他媒体文件时，文本聊天系统应优先推荐全模态模型
4. 文本聊天系统应为全模态模型提供特殊的图标标识以区分纯文本模型
5. 当全模态模型处理多媒体内容时，文本聊天系统应显示相应的处理状态指示器

### 需求12：性能和响应性

**用户故事：** 作为用户，我希望模型切换和文本生成快速且响应迅速，以便我的对话流程不被中断。

#### 验收标准

1. 当用户切换模型时，模型选择器应在100毫秒内更新UI
2. 当发出文本生成请求时，文本聊天系统应在200毫秒内显示加载指示器
3. 当有多个模型可用时，模型选择器应在500毫秒内加载并显示选项
4. 文本聊天系统应缓存模型可用性以避免重复的配置检查
5. 当在模型之间切换时，文本聊天系统应保持平滑的UI过渡而不闪烁