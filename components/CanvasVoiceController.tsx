/**
 * Canvas Voice Controller - ç”»å¸ƒè¯­éŸ³æ§åˆ¶ç»„ä»¶
 * ä½¿ç”¨ç¥é©¬APIçš„Realtimeè¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼Œæ— éœ€è¿æ¥è°·æ­ŒæœåŠ¡
 * æ”¯æŒå®æ—¶è¯­éŸ³è¯†åˆ«å’ŒAIå¯¹è¯
 */

import React, { useState, useRef, useEffect } from 'react';
import { Mic, MicOff, Loader2, MessageCircle, X, Send, ExternalLink } from 'lucide-react';

// ç¥é©¬API Realtime WebSocketæ¥å£
interface RealtimeSession {
  ws: WebSocket | null;
  sessionId: string;
  isConnected: boolean;
}

interface RealtimeEvent {
  type: string;
  event_id?: string;
  session?: any;
  response?: any;
  item?: any;
  audio?: string;
  transcript?: string;
}

// TypeScript declarations for Web Speech API (å¤‡ç”¨æ–¹æ¡ˆ)
declare global {
  interface Window {
    SpeechRecognition: typeof SpeechRecognition;
    webkitSpeechRecognition: typeof SpeechRecognition;
  }
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  start(): void;
  stop(): void;
  onstart: ((this: SpeechRecognition, ev: Event) => any) | null;
  onresult: ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => any) | null;
  onerror: ((this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => any) | null;
  onend: ((this: SpeechRecognition, ev: Event) => any) | null;
}

interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
}

interface SpeechRecognitionErrorEvent extends Event {
  error: string;
}

interface SpeechRecognitionResultList {
  [index: number]: SpeechRecognitionResult;
  length: number;
}

interface SpeechRecognitionResult {
  [index: number]: SpeechRecognitionAlternative;
  length: number;
}

interface SpeechRecognitionAlternative {
  transcript: string;
  confidence: number;
}

declare var SpeechRecognition: {
  prototype: SpeechRecognition;
  new(): SpeechRecognition;
};

interface VoiceCommand {
  command: 'generate_text' | 'generate_image' | 'generate_video' | 'add_to_canvas' | 'unknown';
  text: string;
  content: string;
  params?: {
    aspectRatio?: string;
    style?: string;
    duration?: number;
  };
}

interface ChatMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: number;
  isProcessing?: boolean;
  contentType?: 'text' | 'image' | 'video';
}

interface CanvasVoiceControllerProps {
  onCommand: (command: VoiceCommand) => void;
  lang?: 'zh' | 'en';
  wakeWord?: string;
  position?: { x: number; y: number };
  theme?: 'light' | 'dark';
  isActive?: boolean;
  apiSettings?: {
    provider: string;
    apiKey: string;
    baseUrl: string;
  };
}

const CanvasVoiceController: React.FC<CanvasVoiceControllerProps> = ({
  onCommand,
  lang = 'zh',
  wakeWord = 'æ›¹æ“',
  position = { x: 20, y: 20 },
  theme = 'light',
  isActive = false,
  apiSettings
}) => {
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [showChat, setShowChat] = useState(false);
  const [chatMessages, setChatMessages] = useState<ChatMessage[]>([]);
  const [currentInput, setCurrentInput] = useState('');
  const [error, setError] = useState<string>('');
  const [realtimeSession, setRealtimeSession] = useState<RealtimeSession>({
    ws: null,
    sessionId: '',
    isConnected: false
  });
  const [useRealtimeAPI, setUseRealtimeAPI] = useState<boolean>(true);
  const [audioContext, setAudioContext] = useState<AudioContext | null>(null);
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null);
  
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const chatScrollRef = useRef<HTMLDivElement>(null);
  const audioChunksRef = useRef<Blob[]>([]);

  const t = {
    zh: {
      wakeUp: `ç‚¹å‡»å¼€å§‹è¯­éŸ³å¯¹è¯`,
      listening: 'è¯­éŸ³å¯¹è¯ä¸­...',
      processing: 'æ­£åœ¨å¤„ç†...',
      error: 'é”™è¯¯',
      noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«',
      micPermission: 'éœ€è¦éº¦å…‹é£æƒé™',
      chatTitle: 'è¯­éŸ³å¯¹è¯',
      inputPlaceholder: 'è¾“å…¥æ¶ˆæ¯æˆ–ç‚¹å‡»è¯­éŸ³å¯¹è¯...',
      send: 'å‘é€',
      openRealtimeChat: 'æ‰“å¼€è¯­éŸ³å¯¹è¯',
      realtimeMode: 'å®æ—¶è¯­éŸ³æ¨¡å¼',
      fallbackMode: 'æµè§ˆå™¨è¯­éŸ³æ¨¡å¼',
      apiKeyRequired: 'APIå¯†é’¥æœªé…ç½®'
    },
    en: {
      wakeUp: `Click to start voice chat`,
      listening: 'Voice chatting...',
      processing: 'Processing...',
      error: 'Error',
      noSupport: 'Speech recognition not supported',
      micPermission: 'Microphone permission required',
      chatTitle: 'Voice Chat',
      inputPlaceholder: 'Type message or click voice chat...',
      send: 'Send',
      openRealtimeChat: 'Open Voice Chat',
      realtimeMode: 'Realtime Voice Mode',
      fallbackMode: 'Browser Voice Mode',
      apiKeyRequired: 'API Key Required'
    }
  };

  const currentLang = t[lang];

  // ç›‘å¬æ¿€æ´»çŠ¶æ€å˜åŒ–ï¼Œè‡ªåŠ¨å¼€å§‹/åœæ­¢ç›‘å¬
  useEffect(() => {
    if (isActive && !isListening && !isProcessing) {
      console.log('[CanvasVoiceController] è¯­éŸ³æ§åˆ¶æ¿€æ´»');
      if (useRealtimeAPI && apiSettings?.apiKey && apiSettings.apiKey !== 'PLACEHOLDER_API_KEY') {
        // ä½¿ç”¨ç¥é©¬API Realtimeæ¨¡å¼
        initializeRealtimeChat();
      } else {
        // é™çº§åˆ°æµè§ˆå™¨è¯­éŸ³è¯†åˆ«
        setUseRealtimeAPI(false);
        initializeBrowserSpeech();
      }
    } else if (!isActive && isListening) {
      console.log('[CanvasVoiceController] è¯­éŸ³æ§åˆ¶å…³é—­ï¼Œåœæ­¢ç›‘å¬');
      stopListening();
    }
  }, [isActive, isListening, isProcessing, useRealtimeAPI, apiSettings]);

  // åˆå§‹åŒ–ç¥é©¬API Realtime WebSocketè¿æ¥
  const initializeRealtimeChat = async () => {
    if (!apiSettings?.apiKey || apiSettings.apiKey === 'PLACEHOLDER_API_KEY') {
      setError(currentLang.apiKeyRequired);
      addChatMessage('assistant', `âŒ ${currentLang.apiKeyRequired}ï¼

ğŸ”‘ è¯·å…ˆé…ç½®ç¥é©¬APIå¯†é’¥ï¼š
1. ç‚¹å‡»å³ä¾§è¾¹æ çš„"è®¾ç½®"âš™ï¸æŒ‰é’®
2. åœ¨"APIæä¾›å•†é…ç½®"ä¸­è¾“å…¥ç¥é©¬APIå¯†é’¥
3. ç‚¹å‡»"ä¿å­˜é…ç½®"

ğŸ’¡ é…ç½®å®Œæˆåå³å¯ä½¿ç”¨å®æ—¶è¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼`);
      return;
    }

    try {
      setIsProcessing(true);
      setError('');
      
      console.log('[CanvasVoiceController] åˆå§‹åŒ–ç¥é©¬API Realtime WebSocketè¿æ¥');
      
      // æ„å»ºWebSocket URL - å°†HTTP URLè½¬æ¢ä¸ºWebSocket URL
      const wsUrl = apiSettings.baseUrl.replace('http://', 'ws://').replace('https://', 'wss://') + '/v1/realtime';
      
      console.log('[CanvasVoiceController] è¿æ¥åˆ°:', wsUrl);
      
      // åˆ›å»ºWebSocketè¿æ¥
      const ws = new WebSocket(wsUrl, [], {
        headers: {
          'Authorization': `Bearer ${apiSettings.apiKey}`,
          'OpenAI-Beta': 'realtime=v1'
        }
      } as any);
      
      const sessionId = `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
      
      ws.onopen = () => {
        console.log('[CanvasVoiceController] âœ… WebSocketè¿æ¥å·²å»ºç«‹');
        
        setRealtimeSession({
          ws,
          sessionId,
          isConnected: true
        });
        
        // å‘é€session.updateäº‹ä»¶é…ç½®ä¼šè¯
        const sessionUpdateEvent = {
          event_id: `event_${Date.now()}`,
          type: 'session.update',
          session: {
            modalities: ['text', 'audio'],
            instructions: `ä½ æ˜¯æ›¹æ“ï¼Œä¸€ä¸ªä¸“ä¸šçš„AIç”»å¸ƒåŠ©æ‰‹ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è¯­éŸ³ä¸ä½ å¯¹è¯ï¼Œä½ éœ€è¦ï¼š
1. ç†è§£ç”¨æˆ·çš„åˆ›ä½œéœ€æ±‚
2. å¸®åŠ©ç”¨æˆ·ç”Ÿæˆæ–‡æœ¬ã€å›¾ç‰‡ã€è§†é¢‘å†…å®¹
3. å°†å†…å®¹æ·»åŠ åˆ°ç”»å¸ƒä¸Š
4. ç”¨ç®€æ´å‹å¥½çš„è¯­è¨€å›å¤

å½“ç”¨æˆ·è¦æ±‚ç”Ÿæˆå†…å®¹æ—¶ï¼Œè¯·æ˜ç¡®è¯´æ˜ä½ å°†æ‰§è¡Œçš„æ“ä½œã€‚`,
            voice: 'alloy',
            input_audio_format: 'pcm16',
            output_audio_format: 'pcm16',
            input_audio_transcription: {
              model: 'whisper-1'
            },
            turn_detection: {
              type: 'server_vad',
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 200
            },
            temperature: 0.8,
            max_output_tokens: 'inf'
          }
        };
        
        ws.send(JSON.stringify(sessionUpdateEvent));
        console.log('[CanvasVoiceController] å·²å‘é€session.updateé…ç½®');
        
        setIsListening(true);
        setShowChat(true);
        
        addChatMessage('assistant', `âœ… å®æ—¶è¯­éŸ³å¯¹è¯å·²è¿æ¥ï¼

ğŸ¤ ç°åœ¨ä½ å¯ä»¥ï¼š
1. ç›´æ¥å¯¹ç€éº¦å…‹é£è¯´è¯
2. æˆ‘ä¼šå®æ—¶å¬å–å¹¶å›å¤
3. è¯´å‡ºåˆ›ä½œéœ€æ±‚ï¼Œæˆ‘ä¼šç”Ÿæˆå†…å®¹åˆ°ç”»å¸ƒ

ğŸ’¡ è¯•è¯•è¯´ï¼š"æ›¹æ“ï¼Œå¸®æˆ‘ç”Ÿæˆä¸€å¼ çŒ«çš„å›¾ç‰‡"`);
        
        // å¼€å§‹å½•éŸ³
        startAudioRecording();
      };
      
      ws.onmessage = (event) => {
        try {
          const data: RealtimeEvent = JSON.parse(event.data);
          handleRealtimeEvent(data);
        } catch (error) {
          console.error('[CanvasVoiceController] è§£æWebSocketæ¶ˆæ¯å¤±è´¥:', error);
        }
      };
      
      ws.onerror = (error) => {
        console.error('[CanvasVoiceController] WebSocketé”™è¯¯:', error);
        setError('WebSocketè¿æ¥é”™è¯¯');
        
        addChatMessage('assistant', `âŒ å®æ—¶è¯­éŸ³è¿æ¥å‡ºç°é—®é¢˜

ğŸ”„ æ­£åœ¨å°è¯•é™çº§åˆ°æµè§ˆå™¨è¯­éŸ³æ¨¡å¼...`);
        
        // é™çº§åˆ°æµè§ˆå™¨è¯­éŸ³è¯†åˆ«
        setUseRealtimeAPI(false);
        setTimeout(() => initializeBrowserSpeech(), 1000);
      };
      
      ws.onclose = (event) => {
        console.log('[CanvasVoiceController] WebSocketè¿æ¥å·²å…³é—­:', event.code, event.reason);
        
        setRealtimeSession({
          ws: null,
          sessionId: '',
          isConnected: false
        });
        
        setIsListening(false);
        
        if (event.code !== 1000) { // éæ­£å¸¸å…³é—­
          addChatMessage('assistant', 'ğŸ”Œ è¯­éŸ³è¿æ¥å·²æ–­å¼€ï¼Œç‚¹å‡»é‡æ–°è¿æ¥æˆ–ä½¿ç”¨æ–‡å­—è¾“å…¥ã€‚');
        }
      };
      
    } catch (error) {
      console.error('[CanvasVoiceController] Realtime WebSocketåˆå§‹åŒ–å¤±è´¥:', error);
      setError('Realtimeè¿æ¥åˆå§‹åŒ–å¤±è´¥');
      
      // é™çº§åˆ°æµè§ˆå™¨è¯­éŸ³è¯†åˆ«
      addChatMessage('assistant', `âš ï¸ å®æ—¶è¯­éŸ³è¿æ¥å¤±è´¥ï¼Œå·²åˆ‡æ¢åˆ°æµè§ˆå™¨è¯­éŸ³æ¨¡å¼ã€‚

ğŸ”„ é”™è¯¯ä¿¡æ¯ï¼š${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}

ğŸ’¡ ä½ ä»ç„¶å¯ä»¥ä½¿ç”¨æµè§ˆå™¨çš„è¯­éŸ³è¯†åˆ«åŠŸèƒ½ã€‚`);
      
      setUseRealtimeAPI(false);
      setTimeout(() => initializeBrowserSpeech(), 1000);
    } finally {
      setIsProcessing(false);
    }
  };

  // å¤„ç†Realtimeäº‹ä»¶
  const handleRealtimeEvent = (event: RealtimeEvent) => {
    console.log('[CanvasVoiceController] æ”¶åˆ°Realtimeäº‹ä»¶:', event.type, event);
    
    switch (event.type) {
      case 'session.created':
        console.log('[CanvasVoiceController] ä¼šè¯å·²åˆ›å»º');
        break;
        
      case 'session.updated':
        console.log('[CanvasVoiceController] ä¼šè¯é…ç½®å·²æ›´æ–°');
        break;
        
      case 'input_audio_buffer.speech_started':
        console.log('[CanvasVoiceController] æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹');
        addChatMessage('user', 'ğŸ¤ æ­£åœ¨è¯´è¯...', false, 'voice');
        break;
        
      case 'input_audio_buffer.speech_stopped':
        console.log('[CanvasVoiceController] è¯­éŸ³ç»“æŸ');
        break;
        
      case 'conversation.item.input_audio_transcription.completed':
        if (event.transcript) {
          console.log('[CanvasVoiceController] è¯­éŸ³è½¬å½•å®Œæˆ:', event.transcript);
          // æ›´æ–°ç”¨æˆ·æ¶ˆæ¯æ˜¾ç¤ºè½¬å½•æ–‡æœ¬
          updateLastUserMessage(event.transcript);
        }
        break;
        
      case 'response.created':
        console.log('[CanvasVoiceController] AIå¼€å§‹å“åº”');
        addChatMessage('assistant', 'ğŸ¤” æ­£åœ¨æ€è€ƒ...', true);
        break;
        
      case 'response.output_item.added':
        if (event.item?.type === 'message') {
          console.log('[CanvasVoiceController] AIæ¶ˆæ¯å“åº”');
        }
        break;
        
      case 'response.content_part.added':
        if (event.item?.type === 'text') {
          console.log('[CanvasVoiceController] AIæ–‡æœ¬å†…å®¹');
        }
        break;
        
      case 'response.content_part.done':
        if (event.item?.type === 'text' && event.item.text) {
          console.log('[CanvasVoiceController] AIæ–‡æœ¬å“åº”å®Œæˆ:', event.item.text);
          updateLastAssistantMessage(event.item.text);
          
          // è§£æAIå“åº”ä¸­çš„æŒ‡ä»¤
          parseAndExecuteCommand(event.item.text);
        }
        break;
        
      case 'response.audio.delta':
        if (event.audio) {
          // æ’­æ”¾éŸ³é¢‘ç‰‡æ®µ
          playAudioDelta(event.audio);
        }
        break;
        
      case 'response.done':
        console.log('[CanvasVoiceController] AIå“åº”å®Œæˆ');
        break;
        
      case 'error':
        console.error('[CanvasVoiceController] Realtime APIé”™è¯¯:', event);
        addChatMessage('assistant', `âŒ å‘ç”Ÿé”™è¯¯ï¼š${JSON.stringify(event)}`);
        break;
        
      default:
        console.log('[CanvasVoiceController] æœªå¤„ç†çš„äº‹ä»¶ç±»å‹:', event.type);
    }
  };

  // åˆå§‹åŒ–æµè§ˆå™¨è¯­éŸ³è¯†åˆ«ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
  const initializeBrowserSpeech = () => {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognitionRef.current = new SpeechRecognition();
      
      const recognition = recognitionRef.current;
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.lang = lang === 'zh' ? 'zh-CN' : 'en-US';

      recognition.onstart = () => {
        setIsListening(true);
        setError('');
        console.log('[CanvasVoiceController] æµè§ˆå™¨è¯­éŸ³è¯†åˆ«å¯åŠ¨');
      };

      recognition.onresult = (event: SpeechRecognitionEvent) => {
        const transcript = event.results[0][0].transcript;
        console.log('è¯­éŸ³è¯†åˆ«ç»“æœ:', transcript);
        
        addChatMessage('user', transcript);
        handleVoiceCommand(transcript);
        
        setIsListening(false);
      };

      recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
        setIsListening(false);
        setIsProcessing(false);
        setError(event.error === 'not-allowed' ? currentLang.micPermission : event.error);
        console.error('[CanvasVoiceController] è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error);
      };

      recognition.onend = () => {
        setIsListening(false);
        console.log('[CanvasVoiceController] è¯­éŸ³è¯†åˆ«ç»“æŸ');
      };
      
      // è‡ªåŠ¨å¼€å§‹ç›‘å¬
      startBrowserListening();
    } else {
      setError(currentLang.noSupport);
      addChatMessage('assistant', `âŒ ${currentLang.noSupport}

ğŸ’¡ å»ºè®®ä½¿ç”¨Chromeæˆ–Edgeæµè§ˆå™¨ä»¥è·å¾—æœ€ä½³è¯­éŸ³ä½“éªŒã€‚`);
    }
  };

  // è‡ªåŠ¨æ»šåŠ¨åˆ°èŠå¤©åº•éƒ¨
  useEffect(() => {
    if (chatScrollRef.current) {
      chatScrollRef.current.scrollTop = chatScrollRef.current.scrollHeight;
    }
  }, [chatMessages]);

  const addChatMessage = (role: 'user' | 'assistant', content: string, isProcessing = false) => {
    const message: ChatMessage = {
      id: `voice-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      role,
      content,
      timestamp: Date.now(),
      isProcessing
    };
    setChatMessages((prev: ChatMessage[]) => [...prev, message]);
    return message.id;
  };

  const updateChatMessage = (id: string, content: string, isProcessing = false) => {
    setChatMessages((prev: ChatMessage[]) => prev.map((msg: ChatMessage) => 
      msg.id === id ? { ...msg, content, isProcessing } : msg
    ));
  };

  const startBrowserListening = async () => {
    if (!recognitionRef.current) {
      setError(currentLang.noSupport);
      return;
    }

    try {
      // é¦–å…ˆè¯·æ±‚éº¦å…‹é£æƒé™
      console.log('[CanvasVoiceController] è¯·æ±‚éº¦å…‹é£æƒé™...');
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      
      // æƒé™è·å–æˆåŠŸï¼Œå…³é—­æµï¼ˆæˆ‘ä»¬åªéœ€è¦æƒé™ï¼‰
      stream.getTracks().forEach(track => track.stop());
      console.log('[CanvasVoiceController] âœ… éº¦å…‹é£æƒé™å·²è·å¾—');

      setIsListening(true);
      setError('');
      recognitionRef.current.start();
      setShowChat(true);
      
      addChatMessage('assistant', `ä½ å¥½ï¼æˆ‘æ˜¯${wakeWord}ï¼Œç°åœ¨å¯ä»¥ç›´æ¥è¯´è¯ï¼Œæˆ‘ä¼šå¬å–ä½ çš„æŒ‡ä»¤ã€‚`);
      
      console.log('[CanvasVoiceController] æµè§ˆå™¨è¯­éŸ³è¯†åˆ«å·²å¯åŠ¨');
    } catch (error) {
      console.error('[CanvasVoiceController] å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥:', error);
      if (error instanceof Error && error.name === 'NotAllowedError') {
        setError('éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸éº¦å…‹é£è®¿é—®');
      } else if (error instanceof Error && error.name === 'NotFoundError') {
        setError('æœªæ‰¾åˆ°éº¦å…‹é£è®¾å¤‡');
      } else {
        setError('å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥');
      }
      setIsListening(false);
    }
  };

  const stopListening = () => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop();
      setIsListening(false);
      console.log('[CanvasVoiceController] è¯­éŸ³è¯†åˆ«å·²åœæ­¢');
    }
    
    // åœæ­¢éŸ³é¢‘å½•åˆ¶
    stopAudioRecording();
    
    // å…³é—­WebSocketè¿æ¥
    if (realtimeSession.ws && realtimeSession.isConnected) {
      realtimeSession.ws.close(1000, 'User stopped listening');
      setRealtimeSession({
        ws: null,
        sessionId: '',
        isConnected: false
      });
    }
  };

  // æ›´æ–°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
  const updateLastUserMessage = (transcript: string) => {
    setChatMessages((prev: ChatMessage[]) => {
      const updated = [...prev];
      for (let i = updated.length - 1; i >= 0; i--) {
        if (updated[i].role === 'user' && updated[i].content.includes('ğŸ¤')) {
          updated[i] = {
            ...updated[i],
            content: transcript,
            isProcessing: false
          };
          break;
        }
      }
      return updated;
    });
  };

  // æ›´æ–°æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯
  const updateLastAssistantMessage = (content: string) => {
    setChatMessages((prev: ChatMessage[]) => {
      const updated = [...prev];
      for (let i = updated.length - 1; i >= 0; i--) {
        if (updated[i].role === 'assistant' && updated[i].isProcessing) {
          updated[i] = {
            ...updated[i],
            content,
            isProcessing: false
          };
          break;
        }
      }
      return updated;
    });
  };

  // è§£æAIå“åº”ä¸­çš„æŒ‡ä»¤å¹¶æ‰§è¡Œ
  const parseAndExecuteCommand = (aiResponse: string) => {
    console.log('[CanvasVoiceController] è§£æAIå“åº”:', aiResponse);
    
    // æ£€æµ‹ç”ŸæˆæŒ‡ä»¤çš„å…³é”®è¯
    const lowerResponse = aiResponse.toLowerCase();
    
    let command: VoiceCommand | null = null;
    
    if (lowerResponse.includes('ç”Ÿæˆå›¾ç‰‡') || lowerResponse.includes('ç”»') || lowerResponse.includes('å›¾åƒ')) {
      // æå–å›¾ç‰‡æè¿°
      const imageMatch = aiResponse.match(/(?:ç”Ÿæˆ|ç”»|åˆ¶ä½œ).*?(?:å›¾ç‰‡|å›¾åƒ|ç”»é¢).*?[:ï¼š]?\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)/);
      const content = imageMatch ? imageMatch[1].trim() : aiResponse;
      
      command = {
        command: 'generate_image',
        text: aiResponse,
        content: content,
        params: { aspectRatio: '1:1' }
      };
    } else if (lowerResponse.includes('ç”Ÿæˆè§†é¢‘') || lowerResponse.includes('åˆ¶ä½œè§†é¢‘') || lowerResponse.includes('è§†é¢‘')) {
      // æå–è§†é¢‘æè¿°
      const videoMatch = aiResponse.match(/(?:ç”Ÿæˆ|åˆ¶ä½œ).*?è§†é¢‘.*?[:ï¼š]?\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)/);
      const content = videoMatch ? videoMatch[1].trim() : aiResponse;
      
      command = {
        command: 'generate_video',
        text: aiResponse,
        content: content,
        params: { duration: 10, aspectRatio: '16:9' }
      };
    } else if (lowerResponse.includes('å†™') || lowerResponse.includes('æ–‡æœ¬') || lowerResponse.includes('æ–‡å­—')) {
      // æå–æ–‡æœ¬å†…å®¹
      const textMatch = aiResponse.match(/(?:å†™|ç”Ÿæˆ|åˆ›å»º).*?(?:æ–‡æœ¬|æ–‡å­—|å†…å®¹).*?[:ï¼š]?\s*(.+?)(?:[ã€‚ï¼ï¼Ÿ\n]|$)/);
      const content = textMatch ? textMatch[1].trim() : aiResponse;
      
      command = {
        command: 'generate_text',
        text: aiResponse,
        content: content
      };
    }
    
    // æ‰§è¡ŒæŒ‡ä»¤
    if (command) {
      console.log('[CanvasVoiceController] æ‰§è¡Œè¯­éŸ³æŒ‡ä»¤:', command);
      onCommand(command);
    }
  };

  // å¼€å§‹éŸ³é¢‘å½•åˆ¶
  const startAudioRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        } 
      });
      
      // åˆ›å»ºAudioContextç”¨äºéŸ³é¢‘å¤„ç†
      const audioCtx = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
      setAudioContext(audioCtx);
      
      // åˆ›å»ºMediaRecorderå½•åˆ¶éŸ³é¢‘
      const recorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      recorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
          
          // å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºPCM16æ ¼å¼å¹¶å‘é€åˆ°WebSocket
          convertAndSendAudio(event.data);
        }
      };
      
      recorder.start(100); // æ¯100msæ”¶é›†ä¸€æ¬¡æ•°æ®
      setMediaRecorder(recorder);
      
      console.log('[CanvasVoiceController] âœ… éŸ³é¢‘å½•åˆ¶å·²å¼€å§‹');
      
    } catch (error) {
      console.error('[CanvasVoiceController] éŸ³é¢‘å½•åˆ¶å¯åŠ¨å¤±è´¥:', error);
      addChatMessage('assistant', 'âŒ æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®');
    }
  };

  // è½¬æ¢éŸ³é¢‘æ ¼å¼å¹¶å‘é€åˆ°WebSocket
  const convertAndSendAudio = async (audioBlob: Blob) => {
    if (!realtimeSession.ws || !realtimeSession.isConnected) return;
    
    try {
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioContext = new AudioContext({ sampleRate: 16000 });
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      
      // è½¬æ¢ä¸ºPCM16æ ¼å¼
      const pcm16Data = convertToPCM16(audioBuffer);
      const base64Audio = btoa(String.fromCharCode(...pcm16Data));
      
      // å‘é€éŸ³é¢‘æ•°æ®åˆ°Realtime API
      const audioEvent = {
        event_id: `audio_${Date.now()}`,
        type: 'input_audio_buffer.append',
        audio: base64Audio
      };
      
      realtimeSession.ws.send(JSON.stringify(audioEvent));
      
    } catch (error) {
      console.error('[CanvasVoiceController] éŸ³é¢‘è½¬æ¢å¤±è´¥:', error);
    }
  };

  // è½¬æ¢AudioBufferåˆ°PCM16æ ¼å¼
  const convertToPCM16 = (audioBuffer: AudioBuffer): Uint8Array => {
    const samples = audioBuffer.getChannelData(0);
    const pcm16 = new Int16Array(samples.length);
    
    for (let i = 0; i < samples.length; i++) {
      const sample = Math.max(-1, Math.min(1, samples[i]));
      pcm16[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
    }
    
    return new Uint8Array(pcm16.buffer);
  };

  // æ’­æ”¾AIè¯­éŸ³å›å¤
  const playAudioDelta = async (base64Audio: string) => {
    if (!audioContext) return;
    
    try {
      // è§£ç base64éŸ³é¢‘æ•°æ®
      const binaryString = atob(base64Audio);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }
      
      // è½¬æ¢PCM16æ•°æ®ä¸ºAudioBuffer
      const audioBuffer = audioContext.createBuffer(1, bytes.length / 2, 16000);
      const channelData = audioBuffer.getChannelData(0);
      
      const dataView = new DataView(bytes.buffer);
      for (let i = 0; i < channelData.length; i++) {
        channelData[i] = dataView.getInt16(i * 2, true) / 0x8000;
      }
      
      // æ’­æ”¾éŸ³é¢‘
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);
      source.start();
      
    } catch (error) {
      console.error('[CanvasVoiceController] éŸ³é¢‘æ’­æ”¾å¤±è´¥:', error);
    }
  };

  // åœæ­¢éŸ³é¢‘å½•åˆ¶
  const stopAudioRecording = () => {
    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
      mediaRecorder.stop();
      mediaRecorder.stream.getTracks().forEach(track => track.stop());
      setMediaRecorder(null);
    }
    
    if (audioContext && audioContext.state !== 'closed') {
      audioContext.close();
      setAudioContext(null);
    }
    
    audioChunksRef.current = [];
    console.log('[CanvasVoiceController] éŸ³é¢‘å½•åˆ¶å·²åœæ­¢');
  };

  const handleVoiceCommand = async (transcript: string) => {
    try {
      setIsProcessing(true);
      
      addChatMessage('user', transcript);
      const assistantMsgId = addChatMessage('assistant', 'æ­£åœ¨æ€è€ƒ...', true);
      
      const command = await parseVoiceCommand(transcript);
      
      let responseText = '';
      switch (command.command) {
        case 'generate_text':
          responseText = `å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨ç”Ÿæˆæ–‡æœ¬å†…å®¹ï¼š"${command.content}"`;
          break;
        case 'generate_image':
          responseText = `å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨ç”Ÿæˆå›¾ç‰‡ï¼š"${command.content}"`;
          break;
        case 'generate_video':
          responseText = `å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨ç”Ÿæˆè§†é¢‘ï¼š"${command.content}"`;
          break;
        case 'add_to_canvas':
          responseText = `å¥½çš„ï¼Œæˆ‘æ¥å°†å†…å®¹æ·»åŠ åˆ°ç”»å¸ƒä¸Š`;
          break;
        default:
          responseText = `æˆ‘ç†è§£äº†ä½ çš„éœ€æ±‚ï¼š"${command.content}"ï¼Œè®©æˆ‘æ¥å¤„ç†ä¸€ä¸‹`;
      }
      
      updateChatMessage(assistantMsgId, responseText, false);
      onCommand(command);
      
    } catch (error) {
      console.error('è¯­éŸ³æŒ‡ä»¤å¤„ç†å¤±è´¥:', error);
      const errorMessage = error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯';
      
      // æ£€æŸ¥æ˜¯å¦æ˜¯APIå¯†é’¥é—®é¢˜
      if (errorMessage.includes('PLACEHOLDER_API_KEY') || 
          errorMessage.includes('APIå¯†é’¥æœªé…ç½®') ||
          errorMessage.includes('network') ||
          errorMessage.includes('401') ||
          errorMessage.includes('unauthorized')) {
        addChatMessage('assistant', `âŒ APIå¯†é’¥æœªé…ç½®ï¼

ğŸ”‘ å¿«é€Ÿé…ç½®æ–¹æ³•ï¼š
1. ç‚¹å‡»å³ä¾§è¾¹æ çš„"è®¾ç½®"âš™ï¸æŒ‰é’®
2. åœ¨"APIæä¾›å•†é…ç½®"ä¸­è¾“å…¥ä½ çš„Gemini APIå¯†é’¥
3. ç‚¹å‡»"ä¿å­˜é…ç½®"

ğŸ“ è·å–APIå¯†é’¥ï¼š
â€¢ è®¿é—®ï¼šhttps://aistudio.google.com/app/apikey
â€¢ ç™»å½•Googleè´¦å·å¹¶åˆ›å»ºAPIå¯†é’¥
â€¢ å¤åˆ¶å¯†é’¥åˆ°è®¾ç½®ä¸­

ğŸ’¡ é…ç½®å®Œæˆåï¼Œè¯­éŸ³æ§åˆ¶å°†èƒ½æ­£å¸¸å·¥ä½œï¼`);
      } else {
        addChatMessage('assistant', `æŠ±æ­‰ï¼Œå¤„ç†æŒ‡ä»¤æ—¶å‡ºç°é”™è¯¯ï¼š${errorMessage}

ğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š
â€¢ æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
â€¢ ç¡®è®¤APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®
â€¢ ç¨åé‡è¯•è¯­éŸ³æŒ‡ä»¤

å¦‚éœ€å¸®åŠ©ï¼Œè¯·ç‚¹å‡»å³ä¾§è¾¹æ çš„"è®¾ç½®"æŒ‰é’®æ£€æŸ¥é…ç½®ã€‚`);
      }
    } finally {
      setIsProcessing(false);
    }
  };

  const parseVoiceCommand = async (transcript: string): Promise<VoiceCommand> => {
    const text = transcript.toLowerCase();
    
    if (lang === 'zh') {
      if (text.includes('å†™') || text.includes('æ–‡å­—') || text.includes('æ–‡æœ¬')) {
        return { 
          command: 'generate_text', 
          text: transcript,
          content: transcript.replace(/å†™|æ–‡å­—|æ–‡æœ¬|å¸®æˆ‘|è¯·/g, '').trim()
        };
      } else if (text.includes('è§†é¢‘') || text.includes('å½•åƒ') || text.includes('åŠ¨ç”»')) {
        const duration = text.includes('çŸ­') ? 15 : text.includes('é•¿') ? 60 : 30;
        return { 
          command: 'generate_video', 
          text: transcript,
          content: transcript.replace(/è§†é¢‘|å½•åƒ|åŠ¨ç”»|å¸®æˆ‘|è¯·|ç”Ÿæˆ|åˆ¶ä½œ/g, '').trim(),
          params: { duration }
        };
      } else if (text.includes('å›¾ç‰‡') || text.includes('ç”»') || text.includes('å›¾åƒ') || text.includes('å›¾')) {
        const aspectRatio = text.includes('9:16') || text.includes('ç«–å±') ? '9:16' : 
                           text.includes('16:9') || text.includes('æ¨ªå±') ? '16:9' : '1:1';
        return { 
          command: 'generate_image', 
          text: transcript,
          content: transcript.replace(/å›¾ç‰‡|ç”»|å›¾åƒ|å›¾|å¸®æˆ‘|è¯·|ç”Ÿæˆ|åˆ¶ä½œ/g, '').trim(),
          params: { aspectRatio }
        };
      } else if (text.includes('æ·»åŠ ') || text.includes('æ”¾åˆ°') || text.includes('ç”»å¸ƒ')) {
        return { 
          command: 'add_to_canvas', 
          text: transcript,
          content: transcript.replace(/æ·»åŠ |æ”¾åˆ°|ç”»å¸ƒ|å¸®æˆ‘|è¯·/g, '').trim()
        };
      }
    }
    
    return { 
      command: 'unknown', 
      text: transcript,
      content: transcript
    };
  };

  const handleSendMessage = () => {
    if (!currentInput.trim()) return;
    
    handleVoiceCommand(currentInput);
    setCurrentInput('');
  };

  const handleKeyPress = (e: React.KeyboardEvent<HTMLInputElement>) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSendMessage();
    }
  };

  return (
    <>
      {/* è¯­éŸ³æ§åˆ¶æŒ‰é’® */}
      <div 
        className="fixed z-50 flex items-center gap-2"
        style={{ left: position.x, top: position.y }}
      >
        <button
          onClick={isActive ? stopListening : (useRealtimeAPI ? initializeRealtimeChat : startBrowserListening)}
          disabled={isProcessing}
          className={`
            p-3 rounded-full shadow-lg transition-all duration-300 border-2
            ${isActive && isListening
              ? 'bg-red-500 text-white border-red-600 animate-pulse' 
              : isActive
                ? 'bg-blue-500 text-white border-blue-600'
                : 'bg-white text-gray-700 border-gray-300 hover:bg-gray-50'
            }
            ${isProcessing ? 'opacity-50 cursor-not-allowed' : 'hover:scale-105'}
          `}
          title={
            isActive 
              ? (isListening ? (useRealtimeAPI ? 'å®æ—¶è¯­éŸ³å¯¹è¯ä¸­ - ç‚¹å‡»åœæ­¢' : 'è¿ç»­ç›‘å¬ä¸­ - ç‚¹å‡»åœæ­¢') : 'è¯­éŸ³æ§åˆ¶å·²æ¿€æ´» - ç‚¹å‡»åœæ­¢')
              : (useRealtimeAPI ? 'ç‚¹å‡»å¼€å§‹å®æ—¶è¯­éŸ³å¯¹è¯' : 'ç‚¹å‡»å¼€å§‹è¯­éŸ³æ§åˆ¶')
          }
        >
          {isProcessing ? (
            <Loader2 className="w-5 h-5 animate-spin" />
          ) : isActive && isListening ? (
            <Mic className="w-5 h-5" />
          ) : isActive ? (
            <Mic className="w-5 h-5" />
          ) : (
            <MicOff className="w-5 h-5" />
          )}
        </button>

        {/* çŠ¶æ€æŒ‡ç¤ºå™¨ */}
        {isActive && (
          <div className={`flex items-center gap-1 px-2 py-1 rounded text-xs text-white ${
            useRealtimeAPI ? 'bg-green-500' : 'bg-blue-500'
          }`}>
            <div className="w-2 h-2 bg-white rounded-full animate-pulse"></div>
            {useRealtimeAPI ? (
              realtimeSession.isConnected ? 'å®æ—¶å¯¹è¯' : 'è¿æ¥ä¸­...'
            ) : (
              isListening ? 'ç›‘å¬ä¸­' : 'å·²æ¿€æ´»'
            )}
          </div>
        )}

        {/* æ¨¡å¼åˆ‡æ¢æŒ‰é’® */}
        {isActive && (
          <button
            onClick={() => {
              setUseRealtimeAPI(!useRealtimeAPI);
              addChatMessage('assistant', `å·²åˆ‡æ¢åˆ°${!useRealtimeAPI ? 'å®æ—¶è¯­éŸ³' : 'æµè§ˆå™¨è¯­éŸ³'}æ¨¡å¼ï¼Œè¯·é‡æ–°æ¿€æ´»è¯­éŸ³æ§åˆ¶ã€‚`);
              stopListening();
            }}
            className="p-2 bg-gray-500 text-white rounded-full hover:bg-gray-600 transition-colors text-xs"
            title={useRealtimeAPI ? 'åˆ‡æ¢åˆ°æµè§ˆå™¨è¯­éŸ³æ¨¡å¼' : 'åˆ‡æ¢åˆ°å®æ—¶è¯­éŸ³æ¨¡å¼'}
          >
            {useRealtimeAPI ? 'å®æ—¶' : 'æµè§ˆå™¨'}
          </button>
        )}

        {showChat && (
          <button
            onClick={() => setShowChat(false)}
            className="p-2 bg-gray-500 text-white rounded-full hover:bg-gray-600 transition-colors"
            title="å…³é—­å¯¹è¯"
          >
            <X className="w-4 h-4" />
          </button>
        )}
      </div>

      {/* é”™è¯¯æç¤º */}
      {error && (
        <div 
          className="fixed z-50 bg-red-500 text-white px-4 py-2 rounded-lg shadow-lg"
          style={{ left: position.x, top: position.y + 60 }}
        >
          {error}
        </div>
      )}

      {/* èŠå¤©ç•Œé¢ */}
      {showChat && (
        <div 
          className={`
            fixed z-40 w-80 h-96 rounded-lg shadow-xl border
            ${theme === 'dark' ? 'bg-gray-800 border-gray-600' : 'bg-white border-gray-300'}
          `}
          style={{ left: position.x + 80, top: position.y }}
        >
          {/* èŠå¤©æ ‡é¢˜ */}
          <div className={`p-3 border-b ${theme === 'dark' ? 'border-gray-600' : 'border-gray-200'}`}>
            <div className="flex items-center justify-between">
              <h3 className="font-semibold flex items-center gap-2">
                <MessageCircle className="w-4 h-4" />
                {currentLang.chatTitle}
              </h3>
              <button
                onClick={() => setShowChat(false)}
                className="text-gray-400 hover:text-gray-600"
              >
                <X className="w-4 h-4" />
              </button>
            </div>
          </div>

          {/* æ¶ˆæ¯åˆ—è¡¨ */}
          <div 
            ref={chatScrollRef}
            className="flex-1 overflow-y-auto p-3 space-y-3 h-64"
          >
            {chatMessages.map((message) => (
              <div
                key={message.id}
                className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}
              >
                <div
                  className={`
                    max-w-[80%] p-2 rounded-lg text-sm
                    ${message.role === 'user'
                      ? 'bg-blue-500 text-white'
                      : theme === 'dark'
                        ? 'bg-gray-700 text-white'
                        : 'bg-gray-100 text-gray-800'
                    }
                  `}
                >
                  {message.isProcessing ? (
                    <div className="flex items-center gap-2">
                      <Loader2 className="w-3 h-3 animate-spin" />
                      {message.content}
                    </div>
                  ) : (
                    message.content
                  )}
                </div>
              </div>
            ))}
          </div>

          {/* è¾“å…¥åŒºåŸŸ */}
          <div className={`p-3 border-t ${theme === 'dark' ? 'border-gray-600' : 'border-gray-200'}`}>
            <div className="flex gap-2">
              <input
                type="text"
                value={currentInput}
                onChange={(e) => setCurrentInput(e.target.value)}
                onKeyPress={handleKeyPress}
                placeholder={currentLang.inputPlaceholder}
                className={`
                  flex-1 px-3 py-2 rounded border text-sm
                  ${theme === 'dark'
                    ? 'bg-gray-700 border-gray-600 text-white placeholder-gray-400'
                    : 'bg-white border-gray-300 text-gray-900 placeholder-gray-500'
                  }
                  focus:outline-none focus:ring-2 focus:ring-blue-500
                `}
              />
              <button
                onClick={handleSendMessage}
                disabled={!currentInput.trim()}
                className="px-3 py-2 bg-blue-500 text-white rounded hover:bg-blue-600 disabled:opacity-50 disabled:cursor-not-allowed"
              >
                <Send className="w-4 h-4" />
              </button>
            </div>
          </div>
        </div>
      )}
    </>
  );
};

export default CanvasVoiceController;